{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chat_bot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPD0IXOobCGQUEflpQ6lM2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TGSsaur/AI_Chatbot/blob/master/Chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cen3unzdi7rs",
        "colab_type": "code",
        "outputId": "271c6b2d-2a37-48be-fd3c-15a7184af35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOqTpKrASaB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path =  \"gdrive/My Drive/Project\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQpoZzKWS9su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsen3fO8T4Ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUX8kJZITpps",
        "colab_type": "code",
        "outputId": "d25f25a1-d620-4329-eba7-c3fe8cbae747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip3 install tensorflow==1.0.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/58/b71480f9ec9d08d581d672a81b15ab5fec36a5fcda2093558a23614d8468/tensorflow-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (44.5MB)\n",
            "\u001b[K     |████████████████████████████████| 44.5MB 72kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.18.2)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.0) (46.1.3)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.2.0rc3\n",
            "    Uninstalling tensorflow-2.2.0rc3:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Successfully installed tensorflow-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8Mo7arRUeAF",
        "colab_type": "code",
        "outputId": "84f86b77-e03a-4efe-966d-00b8c26fe950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!apt-get remove cuda\n",
        "!apt-get autoremove cuda\n",
        "!apt-get purge cuda\n",
        "!apt-key del /var/cuda-repo-9-2-local/*.pub\n",
        "!rm -rf /var/cuda-repo-8-0-local-ga2/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h1zGRvhUrre",
        "colab_type": "code",
        "outputId": "228265b3-9d15-4278-c066-bad2d424fc33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!sudo wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-8-0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-24 01:54:06--  https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.199.16.29\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.199.16.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?gCy2VKp8g3skHu0rG2jqNwvfg9lHxTdEBnoSNpuM2ITu3iPKN-zJpmVO8XxFUClEK55oiKBymahWUIa9oLGb8IuAWdO8hWa94O8n14LdtR3TILcydQTZEYVkUW5wWdhvlYrtR9C1Gs6euMxMJHZT_ZkSQ8BoAi11qLbK9mu3xzP1qgWCC3mjDe7WdG9Ni_GoOgPFmrhdtmuXIhP64tI8wFaGUw [following]\n",
            "--2020-04-24 01:54:07--  https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?gCy2VKp8g3skHu0rG2jqNwvfg9lHxTdEBnoSNpuM2ITu3iPKN-zJpmVO8XxFUClEK55oiKBymahWUIa9oLGb8IuAWdO8hWa94O8n14LdtR3TILcydQTZEYVkUW5wWdhvlYrtR9C1Gs6euMxMJHZT_ZkSQ8BoAi11qLbK9mu3xzP1qgWCC3mjDe7WdG9Ni_GoOgPFmrhdtmuXIhP64tI8wFaGUw\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.20.126\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.20.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1913589814 (1.8G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.78G  69.8MB/s    in 25s     \n",
            "\n",
            "2020-04-24 01:54:32 (72.7 MB/s) - ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb’ saved [1913589814/1913589814]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-8-0-local-ga2.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\n",
            "Warning: The postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2\n",
            "Warning: seems to use apt-key (provided by apt) without depending on gnupg or gnupg2.\n",
            "Warning: This will BREAK in the future and should be fixed by the package maintainer(s).\n",
            "Note: Check first if apt-key functionality is needed at all - it probably isn't!\n",
            "Warning: apt-key should not be used in scripts (called from postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2)\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-8-0-local-ga2  InRelease\n",
            "Ign:1 file:/var/cuda-repo-8-0-local-ga2  InRelease\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  Release.gpg [819 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:8 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:10 file:/var/cuda-repo-8-0-local-ga2  Packages [22.7 kB]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [12.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,184 kB]\n",
            "Ign:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,372 kB]\n",
            "Hit:17 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [59.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [7,674 B]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [8,286 B]\n",
            "Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [37.4 kB]\n",
            "Get:22 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,811 kB]\n",
            "Get:23 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [874 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [8,213 B]\n",
            "Get:26 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [839 kB]\n",
            "Get:27 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:29 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [88.1 kB]\n",
            "Get:30 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [889 kB]\n",
            "Get:31 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [44.6 kB]\n",
            "Fetched 7,528 kB in 2s (4,474 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda-8-0 cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "0 upgraded, 31 newly installed, 0 to remove and 90 not upgraded.\n",
            "Need to get 0 B/1,312 MB of archives.\n",
            "After this operation, 2,079 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-8-0-local-ga2  cuda-license-8-0 8.0.61-1 [27.6 kB]\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  cuda-misc-headers-8-0 8.0.61-1 [1,077 kB]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  cuda-core-8-0 8.0.61-1 [20.0 MB]\n",
            "Get:4 file:/var/cuda-repo-8-0-local-ga2  cuda-cudart-8-0 8.0.61-1 [135 kB]\n",
            "Get:5 file:/var/cuda-repo-8-0-local-ga2  cuda-driver-dev-8-0 8.0.61-1 [14.1 kB]\n",
            "Get:6 file:/var/cuda-repo-8-0-local-ga2  cuda-cudart-dev-8-0 8.0.61-1 [1,071 kB]\n",
            "Get:7 file:/var/cuda-repo-8-0-local-ga2  cuda-command-line-tools-8-0 8.0.61-1 [26.1 MB]\n",
            "Get:8 file:/var/cuda-repo-8-0-local-ga2  cuda-nvrtc-8-0 8.0.61-1 [9,585 kB]\n",
            "Get:9 file:/var/cuda-repo-8-0-local-ga2  cuda-nvrtc-dev-8-0 8.0.61-1 [10.8 kB]\n",
            "Get:10 file:/var/cuda-repo-8-0-local-ga2  cuda-cusolver-8-0 8.0.61-1 [29.3 MB]\n",
            "Get:11 file:/var/cuda-repo-8-0-local-ga2  cuda-cusolver-dev-8-0 8.0.61-1 [6,816 kB]\n",
            "Get:12 file:/var/cuda-repo-8-0-local-ga2  cuda-cublas-8-0 8.0.61-1 [27.2 MB]\n",
            "Get:13 file:/var/cuda-repo-8-0-local-ga2  cuda-cublas-dev-8-0 8.0.61-1 [57.4 MB]\n",
            "Get:14 file:/var/cuda-repo-8-0-local-ga2  cuda-cufft-8-0 8.0.61-1 [117 MB]\n",
            "Get:15 file:/var/cuda-repo-8-0-local-ga2  cuda-cufft-dev-8-0 8.0.61-1 [94.8 MB]\n",
            "Get:16 file:/var/cuda-repo-8-0-local-ga2  cuda-curand-8-0 8.0.61-1 [43.7 MB]\n",
            "Get:17 file:/var/cuda-repo-8-0-local-ga2  cuda-curand-dev-8-0 8.0.61-1 [67.7 MB]\n",
            "Get:18 file:/var/cuda-repo-8-0-local-ga2  cuda-cusparse-8-0 8.0.61-1 [28.8 MB]\n",
            "Get:19 file:/var/cuda-repo-8-0-local-ga2  cuda-cusparse-dev-8-0 8.0.61-1 [29.6 MB]\n",
            "Get:20 file:/var/cuda-repo-8-0-local-ga2  cuda-npp-8-0 8.0.61-1 [157 MB]\n",
            "Get:21 file:/var/cuda-repo-8-0-local-ga2  cuda-npp-dev-8-0 8.0.61-1 [82.3 MB]\n",
            "Get:22 file:/var/cuda-repo-8-0-local-ga2  cuda-samples-8-0 8.0.61-1 [101 MB]\n",
            "Get:23 file:/var/cuda-repo-8-0-local-ga2  cuda-documentation-8-0 8.0.61-1 [113 MB]\n",
            "Get:24 file:/var/cuda-repo-8-0-local-ga2  cuda-nvml-dev-8-0 8.0.61-1 [48.4 kB]\n",
            "Get:25 file:/var/cuda-repo-8-0-local-ga2  cuda-nvgraph-8-0 8.0.61-1 [2,948 kB]\n",
            "Get:26 file:/var/cuda-repo-8-0-local-ga2  cuda-nvgraph-dev-8-0 8.0.61-1 [3,028 kB]\n",
            "Get:27 file:/var/cuda-repo-8-0-local-ga2  cuda-visual-tools-8-0 8.0.61-1 [286 MB]\n",
            "Get:28 file:/var/cuda-repo-8-0-local-ga2  cuda-toolkit-8-0 8.0.61-1 [2,892 B]\n",
            "Get:29 file:/var/cuda-repo-8-0-local-ga2  cuda-runtime-8-0 8.0.61-1 [2,574 B]\n",
            "Get:30 file:/var/cuda-repo-8-0-local-ga2  cuda-demo-suite-8-0 8.0.61-1 [4,988 kB]\n",
            "Get:31 file:/var/cuda-repo-8-0-local-ga2  cuda-8-0 8.0.61-1 [2,556 B]\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 31.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cuda-license-8-0.\n",
            "(Reading database ... 144662 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-license-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-8-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-core-8-0.\n",
            "Preparing to unpack .../02-cuda-core-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-core-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-8-0.\n",
            "Preparing to unpack .../03-cuda-cudart-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-8-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-8-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-8-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-8-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-8-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-8-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-8-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cublas-8-0.\n",
            "Preparing to unpack .../11-cuda-cublas-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-8-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cufft-8-0.\n",
            "Preparing to unpack .../13-cuda-cufft-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-8-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-8-0.\n",
            "Preparing to unpack .../15-cuda-curand-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-8-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-8-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-8-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-8-0.\n",
            "Preparing to unpack .../19-cuda-npp-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-8-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-samples-8-0.\n",
            "Preparing to unpack .../21-cuda-samples-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-samples-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-documentation-8-0.\n",
            "Preparing to unpack .../22-cuda-documentation-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-8-0.\n",
            "Preparing to unpack .../23-cuda-nvml-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-8-0.\n",
            "Preparing to unpack .../24-cuda-nvgraph-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-8-0.\n",
            "Preparing to unpack .../25-cuda-nvgraph-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-8-0.\n",
            "Preparing to unpack .../26-cuda-visual-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-8-0.\n",
            "Preparing to unpack .../27-cuda-toolkit-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-runtime-8-0.\n",
            "Preparing to unpack .../28-cuda-runtime-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-8-0.\n",
            "Preparing to unpack .../29-cuda-demo-suite-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-8-0.\n",
            "Preparing to unpack .../30-cuda-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-license-8-0 (8.0.61-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-8.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-core-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-samples-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-8-0 (8.0.61-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2l0zb_wVbgJ",
        "colab_type": "code",
        "outputId": "8f2061fb-d6cd-4c07-e59a-b17e0f8804cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/e2/7389e7a1c10eb209ddabe0b35cca2e522a3985a1df7e07904c76d1d5609c/tensorflow_gpu-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (95.3MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 49kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (1.18.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow-gpu==1.0) (46.1.3)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxNXwalhVklZ",
        "colab_type": "code",
        "outputId": "abcb0346-db81-4997-dc3d-2f3be5c51749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2016 NVIDIA Corporation\n",
            "Built on Tue_Jan_10_13:22:03_CST_2017\n",
            "Cuda compilation tools, release 8.0, V8.0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3cEsDwdVsqI",
        "colab_type": "code",
        "outputId": "c1137b90-86fd-4a0e-9754-746f822c3214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gpu:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad5I09fpVyb3",
        "colab_type": "code",
        "outputId": "f8c5e0aa-2c16-4aa7-bf5b-7952fe9bd781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YASNFYVJjU87",
        "colab_type": "code",
        "outputId": "6007db5b-719a-41a3-9b0a-e21b7440c01b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python chatbot.py"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
            "I tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcudnn.so.5. LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "I tensorflow/stream_executor/cuda/cuda_dnn.cc:3517] Unable to load cuDNN DSO\n",
            "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
            "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
            "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX512F instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            "I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n",
            "name: Tesla P4\n",
            "major: 6 minor: 1 memoryClockRate (GHz) 1.1135\n",
            "pciBusID 0000:00:04.0\n",
            "Total memory: 7.43GiB\n",
            "Free memory: 271.94MiB\n",
            "I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n",
            "I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n",
            "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0)\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 34.5KiB was 32.0KiB, Chunk State: \n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032000000 of size 1280\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032000500 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032000600 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032000700 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032002700 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032002800 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032004800 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032004900 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032006900 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032006a00 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032008a00 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032008b00 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200ab00 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200ac00 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200cc00 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200cd00 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200ce00 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200cf00 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200d000 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200d100 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200f100 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303200f200 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032011200 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032011300 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032013300 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032013400 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032013500 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3032013600 of size 35328\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303201c000 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303201c100 of size 256\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303201c200 of size 8192\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303201e200 of size 8388608\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303281e200 of size 18075648\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303395b200 of size 1048576\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3033a5b200 of size 2048\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f3033a5ba00 of size 12582912\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303465ba00 of size 2097152\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x7f303485ba00 of size 33113600\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: \n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 19 Chunks of size 256 totalling 4.8KiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2048 totalling 2.0KiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 10 Chunks of size 8192 totalling 80.0KiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 35328 totalling 34.5KiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1048576 totalling 1.00MiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2097152 totalling 2.00MiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 8388608 totalling 8.00MiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 12582912 totalling 12.00MiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 18075648 totalling 17.24MiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 33113600 totalling 31.58MiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 71.94MiB\n",
            "I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: \n",
            "Limit:                    75431936\n",
            "InUse:                    75431936\n",
            "MaxInUse:                 75431936\n",
            "NumAllocs:                      38\n",
            "MaxAllocSize:             33113600\n",
            "\n",
            "W tensorflow/core/common_runtime/bfc_allocator.cc:274] *********************************************************************************xxxxxxxxxxxxxxxxxxx\n",
            "W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 34.5KiB.  See logs for memory state.\n",
            "W tensorflow/core/framework/op_kernel.cc:983] Internal: Dst tensor is not initialized.\n",
            "E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Internal: Dst tensor is not initialized.\n",
            "\t [[Node: optimization/zeros_38 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [8825] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\n",
            "    status, run_metadata)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 88, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n",
            "    pywrap_tensorflow.TF_GetCode(status))\n",
            "tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n",
            "\t [[Node: optimization/zeros_38 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [8825] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"chatbot.py\", line 362, in <module>\n",
            "    session.run(tf.global_variables_initializer())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 767, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\n",
            "    feed_dict_string, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n",
            "    target_list, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n",
            "\t [[Node: optimization/zeros_38 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [8825] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
            "\n",
            "Caused by op 'optimization/zeros_38', defined at:\n",
            "  File \"chatbot.py\", line 330, in <module>\n",
            "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 412, in apply_gradients\n",
            "    self._create_slots(var_list)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adam.py\", line 119, in _create_slots\n",
            "    self._zeros_slot(v, \"m\", self._name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 656, in _zeros_slot\n",
            "    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/slot_creator.py\", line 121, in create_zeros_slot\n",
            "    val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 1370, in zeros\n",
            "    output = constant(zero, shape=shape, dtype=dtype, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n",
            "    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n",
            "    original_op=self._default_original_op, op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n",
            "    self._traceback = _extract_stack()\n",
            "\n",
            "InternalError (see above for traceback): Dst tensor is not initialized.\n",
            "\t [[Node: optimization/zeros_38 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [8825] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MCCN9m2awRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        " \n",
        " \n",
        " \n",
        "########## PART 1 - DATA PREPROCESSING ##########\n",
        " \n",
        " \n",
        " \n",
        "# Importing the dataset\n",
        "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        " \n",
        "# Creating a dictionary that maps each line and its id\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]\n",
        " \n",
        "# Creating a list of all of the conversations\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))\n",
        " \n",
        "# Getting separately the questions and the answers\n",
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "    for i in range(len(conversation) - 1):\n",
        "        questions.append(id2line[conversation[i]])\n",
        "        answers.append(id2line[conversation[i+1]])\n",
        " \n",
        "# Doing a first cleaning of the texts\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text\n",
        " \n",
        "# Cleaning the questions\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        " \n",
        "# Cleaning the answers\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))\n",
        "\n",
        "# Filtering out the questions and answers that are too short or too long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if 2 <= len(question.split()) <= 25:\n",
        "        short_questions.append(question)\n",
        "        short_answers.append(clean_answers[i])\n",
        "    i += 1\n",
        "clean_questions = []\n",
        "clean_answers = []\n",
        "i = 0\n",
        "for answer in short_answers:\n",
        "    if 2 <= len(answer.split()) <= 25:\n",
        "        clean_answers.append(answer)\n",
        "        clean_questions.append(short_questions[i])\n",
        "    i += 1\n",
        " \n",
        "# Creating a dictionary that maps each word to its number of occurrences\n",
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        " \n",
        "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
        "threshold_questions = 15\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "threshold_answers = 15\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "# Adding the last tokens to these two dictionaries\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) + 1\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) + 1\n",
        " \n",
        "# Creating the inverse dictionary of the answerswords2int dictionary\n",
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}\n",
        " \n",
        "# Adding the End Of String token to the end of every answer\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += ' <EOS>'\n",
        " \n",
        "# Translating all the questions and the answers into integers\n",
        "# and Replacing all the words that were filtered out by <OUT> \n",
        "questions_into_int = []\n",
        "for question in clean_questions:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "answers_into_int = []\n",
        "for answer in clean_answers:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)\n",
        " \n",
        "# Sorting questions and answers by the length of questions\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])\n",
        " \n",
        " \n",
        " \n",
        "########## PART 2 - BUILDING THE SEQ2SEQ MODEL ##########\n",
        " \n",
        " \n",
        " \n",
        "# Creating placeholders for the inputs and the targets\n",
        "def model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
        "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
        "    return inputs, targets, lr, keep_prob\n",
        " \n",
        "# Preprocessing the targets\n",
        "def preprocess_targets(targets, word2int, batch_size):\n",
        "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
        "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
        "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
        "    return preprocessed_targets\n",
        " \n",
        "# Creating the Encoder RNN\n",
        "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
        "                                                                    cell_bw = encoder_cell,\n",
        "                                                                    sequence_length = sequence_length,\n",
        "                                                                    inputs = rnn_inputs,\n",
        "                                                                    dtype = tf.float32)\n",
        "    return encoder_state\n",
        " \n",
        "# Decoding the training set\n",
        "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              name = \"attn_dec_train\")\n",
        "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                              training_decoder_function,\n",
        "                                                                                                              decoder_embedded_input,\n",
        "                                                                                                              sequence_length,\n",
        "                                                                                                              scope = decoding_scope)\n",
        "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
        "    return output_function(decoder_output_dropout)\n",
        " \n",
        "# Decoding the test/validation set\n",
        "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
        "                                                                              encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              decoder_embeddings_matrix,\n",
        "                                                                              sos_id,\n",
        "                                                                              eos_id,\n",
        "                                                                              maximum_length,\n",
        "                                                                              num_words,\n",
        "                                                                              name = \"attn_dec_inf\")\n",
        "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                                test_decoder_function,\n",
        "                                                                                                                scope = decoding_scope)\n",
        "    return test_predictions\n",
        " \n",
        "# Creating the Decoder RNN\n",
        "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
        "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
        "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
        "        biases = tf.zeros_initializer()\n",
        "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
        "                                                                      num_words,\n",
        "                                                                      None,\n",
        "                                                                      scope = decoding_scope,\n",
        "                                                                      weights_initializer = weights,\n",
        "                                                                      biases_initializer = biases)\n",
        "        training_predictions = decode_training_set(encoder_state,\n",
        "                                                   decoder_cell,\n",
        "                                                   decoder_embedded_input,\n",
        "                                                   sequence_length,\n",
        "                                                   decoding_scope,\n",
        "                                                   output_function,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size)\n",
        "        decoding_scope.reuse_variables()\n",
        "        test_predictions = decode_test_set(encoder_state,\n",
        "                                           decoder_cell,\n",
        "                                           decoder_embeddings_matrix,\n",
        "                                           word2int['<SOS>'],\n",
        "                                           word2int['<EOS>'],\n",
        "                                           sequence_length - 1,\n",
        "                                           num_words,\n",
        "                                           decoding_scope,\n",
        "                                           output_function,\n",
        "                                           keep_prob,\n",
        "                                           batch_size)\n",
        "    return training_predictions, test_predictions\n",
        " \n",
        "# Building the seq2seq model\n",
        "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
        "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
        "                                                              answers_num_words + 1,\n",
        "                                                              encoder_embedding_size,\n",
        "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
        "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
        "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
        "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
        "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
        "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
        "                                                         decoder_embeddings_matrix,\n",
        "                                                         encoder_state,\n",
        "                                                         questions_num_words,\n",
        "                                                         sequence_length,\n",
        "                                                         rnn_size,\n",
        "                                                         num_layers,\n",
        "                                                         questionswords2int,\n",
        "                                                         keep_prob,\n",
        "                                                         batch_size)\n",
        "    return training_predictions, test_predictions\n",
        " \n",
        " \n",
        " \n",
        "########## PART 3 - TRAINING THE SEQ2SEQ MODEL ##########\n",
        " \n",
        " \n",
        " \n",
        "# Setting the Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "rnn_size = 1024\n",
        "num_layers = 3\n",
        "encoding_embedding_size = 1024\n",
        "decoding_embedding_size = 1024\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0.9\n",
        "min_learning_rate = 0.0001\n",
        "keep_probability = 0.5\n",
        " \n",
        "# Defining a session\n",
        "tf.reset_default_graph()\n",
        "session = tf.InteractiveSession()\n",
        " \n",
        "# Loading the model inputs\n",
        "inputs, targets, lr, keep_prob = model_inputs()\n",
        " \n",
        "# Setting the sequence length\n",
        "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
        " \n",
        "# Getting the shape of the inputs tensor\n",
        "input_shape = tf.shape(inputs)\n",
        " \n",
        "# Getting the training and test predictions\n",
        "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                       targets,\n",
        "                                                       keep_prob,\n",
        "                                                       batch_size,\n",
        "                                                       sequence_length,\n",
        "                                                       len(answerswords2int),\n",
        "                                                       len(questionswords2int),\n",
        "                                                       encoding_embedding_size,\n",
        "                                                       decoding_embedding_size,\n",
        "                                                       rnn_size,\n",
        "                                                       num_layers,\n",
        "                                                       questionswords2int)\n",
        " \n",
        "# Setting up the Loss Error, the Optimizer and Gradient Clipping\n",
        "with tf.name_scope(\"optimization\"):\n",
        "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
        "                                                  targets,\n",
        "                                                  tf.ones([input_shape[0], sequence_length]))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    gradients = optimizer.compute_gradients(loss_error)\n",
        "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
        "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)\n",
        " \n",
        "# Padding the sequences with the <PAD> token\n",
        "def apply_padding(batch_of_sequences, word2int):\n",
        "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
        " \n",
        "# Splitting the data into batches of questions and answers\n",
        "def split_into_batches(questions, answers, batch_size):\n",
        "    for batch_index in range(0, len(questions) // batch_size):\n",
        "        start_index = batch_index * batch_size\n",
        "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
        "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
        "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
        "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
        "        yield padded_questions_in_batch, padded_answers_in_batch\n",
        " \n",
        "# Splitting the questions and answers into training and validation sets\n",
        "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
        "training_questions = sorted_clean_questions[training_validation_split:]\n",
        "training_answers = sorted_clean_answers[training_validation_split:]\n",
        "validation_questions = sorted_clean_questions[:training_validation_split]\n",
        "validation_answers = sorted_clean_answers[:training_validation_split]\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWV1a48SJjpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "batch_index_check_training_loss = 100\n",
        "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
        "total_training_loss_error = 0\n",
        "list_validation_loss_error = [1.746]\n",
        "early_stopping_check = 0\n",
        "early_stopping_stop = 1000\n",
        "checkpoint = \"./chatbot_weights.ckpt\" # For Windows users, replace this line of code by: checkpoint = \"./chatbot_weights.ckpt\"\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOgXb7RfkX5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = \"./chatbot_weights.ckpt\"\n",
        "session = tf.InteractiveSession()\n",
        "session.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(session, checkpoint)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKxhVqzFk05c",
        "colab_type": "code",
        "outputId": "57c409a7-7b82-4c3e-cb31-bbd7fc9610d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(56, epochs + 1):\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
        "        starting_time = time.time()\n",
        "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
        "                                                                                               targets: padded_answers_in_batch,\n",
        "                                                                                               lr: learning_rate,\n",
        "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                                               keep_prob: keep_probability})\n",
        "        total_training_loss_error += batch_training_loss_error\n",
        "        ending_time = time.time()\n",
        "        batch_time = ending_time - starting_time\n",
        "        if batch_index % batch_index_check_training_loss == 0:\n",
        "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
        "                                                                                                                                       epochs,\n",
        "                                                                                                                                       batch_index,\n",
        "                                                                                                                                       len(training_questions) // batch_size,\n",
        "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
        "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
        "            total_training_loss_error = 0\n",
        "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
        "            total_validation_loss_error = 0\n",
        "            starting_time = time.time()\n",
        "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
        "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
        "                                                                       targets: padded_answers_in_batch,\n",
        "                                                                       lr: learning_rate,\n",
        "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                       keep_prob: 1})\n",
        "                total_validation_loss_error += batch_validation_loss_error\n",
        "            ending_time = time.time()\n",
        "            batch_time = ending_time - starting_time\n",
        "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
        "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
        "            learning_rate *= learning_rate_decay\n",
        "            if learning_rate < min_learning_rate:\n",
        "                learning_rate = min_learning_rate\n",
        "            list_validation_loss_error.append(average_validation_loss_error)\n",
        "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
        "                print('I speak better now!!')\n",
        "                early_stopping_check = 0\n",
        "                saver = tf.train.Saver()\n",
        "                saver.save(session, checkpoint)\n",
        "            else:\n",
        "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
        "                early_stopping_check += 1\n",
        "                if early_stopping_check == early_stopping_stop:\n",
        "                    break\n",
        "    if early_stopping_check == early_stopping_stop:\n",
        "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
        "        break\n",
        "print(\"Game Over\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  56/100, Batch:    0/4120, Training Loss Error:  0.012, Training Time on 100 Batches: 5965 seconds\n",
            "Epoch:  56/100, Batch:  100/4120, Training Loss Error:  1.629, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  56/100, Batch:  200/4120, Training Loss Error:  1.625, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  56/100, Batch:  300/4120, Training Loss Error:  1.611, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  56/100, Batch:  400/4120, Training Loss Error:  1.566, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  56/100, Batch:  500/4120, Training Loss Error:  1.614, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  56/100, Batch:  600/4120, Training Loss Error:  1.599, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  56/100, Batch:  700/4120, Training Loss Error:  1.590, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  56/100, Batch:  800/4120, Training Loss Error:  1.616, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  56/100, Batch:  900/4120, Training Loss Error:  1.619, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  56/100, Batch: 1000/4120, Training Loss Error:  1.624, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  56/100, Batch: 1100/4120, Training Loss Error:  1.636, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  56/100, Batch: 1200/4120, Training Loss Error:  1.612, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  56/100, Batch: 1300/4120, Training Loss Error:  1.629, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  56/100, Batch: 1400/4120, Training Loss Error:  1.589, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  56/100, Batch: 1500/4120, Training Loss Error:  1.619, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  56/100, Batch: 1600/4120, Training Loss Error:  1.611, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  56/100, Batch: 1700/4120, Training Loss Error:  1.640, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  56/100, Batch: 1800/4120, Training Loss Error:  1.639, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  56/100, Batch: 1900/4120, Training Loss Error:  1.606, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  56/100, Batch: 2000/4120, Training Loss Error:  1.616, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.761, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  56/100, Batch: 2100/4120, Training Loss Error:  1.630, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  56/100, Batch: 2200/4120, Training Loss Error:  1.774, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  56/100, Batch: 2300/4120, Training Loss Error:  1.714, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  56/100, Batch: 2400/4120, Training Loss Error:  1.692, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  56/100, Batch: 2500/4120, Training Loss Error:  1.728, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  56/100, Batch: 2600/4120, Training Loss Error:  1.724, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  56/100, Batch: 2700/4120, Training Loss Error:  1.738, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  56/100, Batch: 2800/4120, Training Loss Error:  1.731, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  56/100, Batch: 2900/4120, Training Loss Error:  1.716, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  56/100, Batch: 3000/4120, Training Loss Error:  1.707, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  56/100, Batch: 3100/4120, Training Loss Error:  1.779, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  56/100, Batch: 3200/4120, Training Loss Error:  1.666, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  56/100, Batch: 3300/4120, Training Loss Error:  1.691, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  56/100, Batch: 3400/4120, Training Loss Error:  1.726, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  56/100, Batch: 3500/4120, Training Loss Error:  1.692, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  56/100, Batch: 3600/4120, Training Loss Error:  1.690, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  56/100, Batch: 3700/4120, Training Loss Error:  1.724, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  56/100, Batch: 3800/4120, Training Loss Error:  1.675, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  56/100, Batch: 3900/4120, Training Loss Error:  1.727, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  56/100, Batch: 4000/4120, Training Loss Error:  1.709, Training Time on 100 Batches: 37 seconds\n",
            "Epoch:  56/100, Batch: 4100/4120, Training Loss Error:  1.719, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.774, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  57/100, Batch:    0/4120, Training Loss Error:  0.330, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  57/100, Batch:  100/4120, Training Loss Error:  1.633, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  57/100, Batch:  200/4120, Training Loss Error:  1.627, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  57/100, Batch:  300/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  57/100, Batch:  400/4120, Training Loss Error:  1.568, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  57/100, Batch:  500/4120, Training Loss Error:  1.615, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  57/100, Batch:  600/4120, Training Loss Error:  1.593, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  57/100, Batch:  700/4120, Training Loss Error:  1.582, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  57/100, Batch:  800/4120, Training Loss Error:  1.616, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  57/100, Batch:  900/4120, Training Loss Error:  1.619, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  57/100, Batch: 1000/4120, Training Loss Error:  1.622, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  57/100, Batch: 1100/4120, Training Loss Error:  1.636, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  57/100, Batch: 1200/4120, Training Loss Error:  1.613, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  57/100, Batch: 1300/4120, Training Loss Error:  1.627, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  57/100, Batch: 1400/4120, Training Loss Error:  1.592, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  57/100, Batch: 1500/4120, Training Loss Error:  1.616, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  57/100, Batch: 1600/4120, Training Loss Error:  1.609, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  57/100, Batch: 1700/4120, Training Loss Error:  1.634, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  57/100, Batch: 1800/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  57/100, Batch: 1900/4120, Training Loss Error:  1.603, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  57/100, Batch: 2000/4120, Training Loss Error:  1.615, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.756, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  57/100, Batch: 2100/4120, Training Loss Error:  1.622, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  57/100, Batch: 2200/4120, Training Loss Error:  1.748, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  57/100, Batch: 2300/4120, Training Loss Error:  1.694, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  57/100, Batch: 2400/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  57/100, Batch: 2500/4120, Training Loss Error:  1.705, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  57/100, Batch: 2600/4120, Training Loss Error:  1.703, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  57/100, Batch: 2700/4120, Training Loss Error:  1.721, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  57/100, Batch: 2800/4120, Training Loss Error:  1.708, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  57/100, Batch: 2900/4120, Training Loss Error:  1.700, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  57/100, Batch: 3000/4120, Training Loss Error:  1.686, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  57/100, Batch: 3100/4120, Training Loss Error:  1.759, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  57/100, Batch: 3200/4120, Training Loss Error:  1.649, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  57/100, Batch: 3300/4120, Training Loss Error:  1.671, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  57/100, Batch: 3400/4120, Training Loss Error:  1.704, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  57/100, Batch: 3500/4120, Training Loss Error:  1.673, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  57/100, Batch: 3600/4120, Training Loss Error:  1.670, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  57/100, Batch: 3700/4120, Training Loss Error:  1.707, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  57/100, Batch: 3800/4120, Training Loss Error:  1.653, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  57/100, Batch: 3900/4120, Training Loss Error:  1.707, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  57/100, Batch: 4000/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  57/100, Batch: 4100/4120, Training Loss Error:  1.700, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.780, Batch Validation Time: 49 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  58/100, Batch:    0/4120, Training Loss Error:  0.326, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  58/100, Batch:  100/4120, Training Loss Error:  1.623, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  58/100, Batch:  200/4120, Training Loss Error:  1.620, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  58/100, Batch:  300/4120, Training Loss Error:  1.605, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  58/100, Batch:  400/4120, Training Loss Error:  1.558, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  58/100, Batch:  500/4120, Training Loss Error:  1.604, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  58/100, Batch:  600/4120, Training Loss Error:  1.584, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  58/100, Batch:  700/4120, Training Loss Error:  1.581, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  58/100, Batch:  800/4120, Training Loss Error:  1.603, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  58/100, Batch:  900/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  58/100, Batch: 1000/4120, Training Loss Error:  1.612, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  58/100, Batch: 1100/4120, Training Loss Error:  1.627, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  58/100, Batch: 1200/4120, Training Loss Error:  1.601, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  58/100, Batch: 1300/4120, Training Loss Error:  1.618, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  58/100, Batch: 1400/4120, Training Loss Error:  1.583, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  58/100, Batch: 1500/4120, Training Loss Error:  1.612, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  58/100, Batch: 1600/4120, Training Loss Error:  1.599, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  58/100, Batch: 1700/4120, Training Loss Error:  1.626, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  58/100, Batch: 1800/4120, Training Loss Error:  1.628, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  58/100, Batch: 1900/4120, Training Loss Error:  1.596, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  58/100, Batch: 2000/4120, Training Loss Error:  1.605, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.758, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  58/100, Batch: 2100/4120, Training Loss Error:  1.608, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  58/100, Batch: 2200/4120, Training Loss Error:  1.730, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  58/100, Batch: 2300/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  58/100, Batch: 2400/4120, Training Loss Error:  1.659, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  58/100, Batch: 2500/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  58/100, Batch: 2600/4120, Training Loss Error:  1.687, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  58/100, Batch: 2700/4120, Training Loss Error:  1.702, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  58/100, Batch: 2800/4120, Training Loss Error:  1.693, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  58/100, Batch: 2900/4120, Training Loss Error:  1.683, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  58/100, Batch: 3000/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  58/100, Batch: 3100/4120, Training Loss Error:  1.743, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  58/100, Batch: 3200/4120, Training Loss Error:  1.632, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  58/100, Batch: 3300/4120, Training Loss Error:  1.655, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  58/100, Batch: 3400/4120, Training Loss Error:  1.691, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  58/100, Batch: 3500/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  58/100, Batch: 3600/4120, Training Loss Error:  1.652, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  58/100, Batch: 3700/4120, Training Loss Error:  1.691, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  58/100, Batch: 3800/4120, Training Loss Error:  1.639, Training Time on 100 Batches: 39 seconds\n",
            "Epoch:  58/100, Batch: 3900/4120, Training Loss Error:  1.693, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  58/100, Batch: 4000/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  58/100, Batch: 4100/4120, Training Loss Error:  1.683, Training Time on 100 Batches: 40 seconds\n",
            "Validation Loss Error:  1.792, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  59/100, Batch:    0/4120, Training Loss Error:  0.323, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  59/100, Batch:  100/4120, Training Loss Error:  1.615, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  59/100, Batch:  200/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  59/100, Batch:  300/4120, Training Loss Error:  1.592, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  59/100, Batch:  400/4120, Training Loss Error:  1.548, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  59/100, Batch:  500/4120, Training Loss Error:  1.596, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  59/100, Batch:  600/4120, Training Loss Error:  1.574, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  59/100, Batch:  700/4120, Training Loss Error:  1.568, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  59/100, Batch:  800/4120, Training Loss Error:  1.596, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  59/100, Batch:  900/4120, Training Loss Error:  1.596, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  59/100, Batch: 1000/4120, Training Loss Error:  1.601, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  59/100, Batch: 1100/4120, Training Loss Error:  1.616, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  59/100, Batch: 1200/4120, Training Loss Error:  1.592, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  59/100, Batch: 1300/4120, Training Loss Error:  1.607, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  59/100, Batch: 1400/4120, Training Loss Error:  1.571, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  59/100, Batch: 1500/4120, Training Loss Error:  1.601, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  59/100, Batch: 1600/4120, Training Loss Error:  1.591, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  59/100, Batch: 1700/4120, Training Loss Error:  1.618, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  59/100, Batch: 1800/4120, Training Loss Error:  1.616, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  59/100, Batch: 1900/4120, Training Loss Error:  1.584, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  59/100, Batch: 2000/4120, Training Loss Error:  1.595, Training Time on 100 Batches: 30 seconds\n",
            "Validation Loss Error:  1.763, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  59/100, Batch: 2100/4120, Training Loss Error:  1.593, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  59/100, Batch: 2200/4120, Training Loss Error:  1.714, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  59/100, Batch: 2300/4120, Training Loss Error:  1.664, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  59/100, Batch: 2400/4120, Training Loss Error:  1.647, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  59/100, Batch: 2500/4120, Training Loss Error:  1.676, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  59/100, Batch: 2600/4120, Training Loss Error:  1.676, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  59/100, Batch: 2700/4120, Training Loss Error:  1.690, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  59/100, Batch: 2800/4120, Training Loss Error:  1.682, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  59/100, Batch: 2900/4120, Training Loss Error:  1.670, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  59/100, Batch: 3000/4120, Training Loss Error:  1.659, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  59/100, Batch: 3100/4120, Training Loss Error:  1.730, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  59/100, Batch: 3200/4120, Training Loss Error:  1.618, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  59/100, Batch: 3300/4120, Training Loss Error:  1.644, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  59/100, Batch: 3400/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  59/100, Batch: 3500/4120, Training Loss Error:  1.643, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  59/100, Batch: 3600/4120, Training Loss Error:  1.641, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  59/100, Batch: 3700/4120, Training Loss Error:  1.678, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  59/100, Batch: 3800/4120, Training Loss Error:  1.629, Training Time on 100 Batches: 37 seconds\n",
            "Epoch:  59/100, Batch: 3900/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  59/100, Batch: 4000/4120, Training Loss Error:  1.662, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  59/100, Batch: 4100/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.799, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  60/100, Batch:    0/4120, Training Loss Error:  0.321, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  60/100, Batch:  100/4120, Training Loss Error:  1.607, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  60/100, Batch:  200/4120, Training Loss Error:  1.600, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  60/100, Batch:  300/4120, Training Loss Error:  1.584, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  60/100, Batch:  400/4120, Training Loss Error:  1.537, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  60/100, Batch:  500/4120, Training Loss Error:  1.583, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  60/100, Batch:  600/4120, Training Loss Error:  1.561, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  60/100, Batch:  700/4120, Training Loss Error:  1.559, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  60/100, Batch:  800/4120, Training Loss Error:  1.584, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  60/100, Batch:  900/4120, Training Loss Error:  1.588, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  60/100, Batch: 1000/4120, Training Loss Error:  1.593, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  60/100, Batch: 1100/4120, Training Loss Error:  1.603, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  60/100, Batch: 1200/4120, Training Loss Error:  1.583, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  60/100, Batch: 1300/4120, Training Loss Error:  1.600, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  60/100, Batch: 1400/4120, Training Loss Error:  1.562, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  60/100, Batch: 1500/4120, Training Loss Error:  1.588, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  60/100, Batch: 1600/4120, Training Loss Error:  1.578, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  60/100, Batch: 1700/4120, Training Loss Error:  1.604, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  60/100, Batch: 1800/4120, Training Loss Error:  1.602, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  60/100, Batch: 1900/4120, Training Loss Error:  1.574, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  60/100, Batch: 2000/4120, Training Loss Error:  1.582, Training Time on 100 Batches: 30 seconds\n",
            "Validation Loss Error:  1.769, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  60/100, Batch: 2100/4120, Training Loss Error:  1.582, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  60/100, Batch: 2200/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  60/100, Batch: 2300/4120, Training Loss Error:  1.652, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  60/100, Batch: 2400/4120, Training Loss Error:  1.631, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  60/100, Batch: 2500/4120, Training Loss Error:  1.663, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  60/100, Batch: 2600/4120, Training Loss Error:  1.665, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  60/100, Batch: 2700/4120, Training Loss Error:  1.675, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  60/100, Batch: 2800/4120, Training Loss Error:  1.669, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  60/100, Batch: 2900/4120, Training Loss Error:  1.655, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  60/100, Batch: 3000/4120, Training Loss Error:  1.647, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  60/100, Batch: 3100/4120, Training Loss Error:  1.715, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  60/100, Batch: 3200/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  60/100, Batch: 3300/4120, Training Loss Error:  1.629, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  60/100, Batch: 3400/4120, Training Loss Error:  1.665, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  60/100, Batch: 3500/4120, Training Loss Error:  1.628, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  60/100, Batch: 3600/4120, Training Loss Error:  1.627, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  60/100, Batch: 3700/4120, Training Loss Error:  1.663, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  60/100, Batch: 3800/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  60/100, Batch: 3900/4120, Training Loss Error:  1.663, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  60/100, Batch: 4000/4120, Training Loss Error:  1.648, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  60/100, Batch: 4100/4120, Training Loss Error:  1.659, Training Time on 100 Batches: 40 seconds\n",
            "Validation Loss Error:  1.805, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  61/100, Batch:    0/4120, Training Loss Error:  0.319, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  61/100, Batch:  100/4120, Training Loss Error:  1.595, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  61/100, Batch:  200/4120, Training Loss Error:  1.584, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  61/100, Batch:  300/4120, Training Loss Error:  1.570, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  61/100, Batch:  400/4120, Training Loss Error:  1.525, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  61/100, Batch:  500/4120, Training Loss Error:  1.570, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  61/100, Batch:  600/4120, Training Loss Error:  1.546, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  61/100, Batch:  700/4120, Training Loss Error:  1.544, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  61/100, Batch:  800/4120, Training Loss Error:  1.567, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  61/100, Batch:  900/4120, Training Loss Error:  1.572, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  61/100, Batch: 1000/4120, Training Loss Error:  1.578, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  61/100, Batch: 1100/4120, Training Loss Error:  1.592, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  61/100, Batch: 1200/4120, Training Loss Error:  1.573, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  61/100, Batch: 1300/4120, Training Loss Error:  1.587, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  61/100, Batch: 1400/4120, Training Loss Error:  1.549, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  61/100, Batch: 1500/4120, Training Loss Error:  1.579, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  61/100, Batch: 1600/4120, Training Loss Error:  1.566, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  61/100, Batch: 1700/4120, Training Loss Error:  1.594, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  61/100, Batch: 1800/4120, Training Loss Error:  1.598, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  61/100, Batch: 1900/4120, Training Loss Error:  1.556, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  61/100, Batch: 2000/4120, Training Loss Error:  1.571, Training Time on 100 Batches: 30 seconds\n",
            "Validation Loss Error:  1.773, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  61/100, Batch: 2100/4120, Training Loss Error:  1.571, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  61/100, Batch: 2200/4120, Training Loss Error:  1.685, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  61/100, Batch: 2300/4120, Training Loss Error:  1.642, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  61/100, Batch: 2400/4120, Training Loss Error:  1.622, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  61/100, Batch: 2500/4120, Training Loss Error:  1.649, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  61/100, Batch: 2600/4120, Training Loss Error:  1.651, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  61/100, Batch: 2700/4120, Training Loss Error:  1.664, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  61/100, Batch: 2800/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  61/100, Batch: 2900/4120, Training Loss Error:  1.645, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  61/100, Batch: 3000/4120, Training Loss Error:  1.635, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  61/100, Batch: 3100/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  61/100, Batch: 3200/4120, Training Loss Error:  1.597, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  61/100, Batch: 3300/4120, Training Loss Error:  1.616, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  61/100, Batch: 3400/4120, Training Loss Error:  1.648, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  61/100, Batch: 3500/4120, Training Loss Error:  1.620, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  61/100, Batch: 3600/4120, Training Loss Error:  1.613, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  61/100, Batch: 3700/4120, Training Loss Error:  1.649, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  61/100, Batch: 3800/4120, Training Loss Error:  1.600, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  61/100, Batch: 3900/4120, Training Loss Error:  1.650, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  61/100, Batch: 4000/4120, Training Loss Error:  1.635, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  61/100, Batch: 4100/4120, Training Loss Error:  1.643, Training Time on 100 Batches: 40 seconds\n",
            "Validation Loss Error:  1.810, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  62/100, Batch:    0/4120, Training Loss Error:  0.315, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  62/100, Batch:  100/4120, Training Loss Error:  1.586, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  62/100, Batch:  200/4120, Training Loss Error:  1.575, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  62/100, Batch:  300/4120, Training Loss Error:  1.558, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  62/100, Batch:  400/4120, Training Loss Error:  1.509, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  62/100, Batch:  500/4120, Training Loss Error:  1.555, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  62/100, Batch:  600/4120, Training Loss Error:  1.533, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  62/100, Batch:  700/4120, Training Loss Error:  1.530, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  62/100, Batch:  800/4120, Training Loss Error:  1.560, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  62/100, Batch:  900/4120, Training Loss Error:  1.562, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  62/100, Batch: 1000/4120, Training Loss Error:  1.574, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  62/100, Batch: 1100/4120, Training Loss Error:  1.582, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  62/100, Batch: 1200/4120, Training Loss Error:  1.557, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  62/100, Batch: 1300/4120, Training Loss Error:  1.577, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  62/100, Batch: 1400/4120, Training Loss Error:  1.535, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  62/100, Batch: 1500/4120, Training Loss Error:  1.567, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  62/100, Batch: 1600/4120, Training Loss Error:  1.552, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  62/100, Batch: 1700/4120, Training Loss Error:  1.578, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  62/100, Batch: 1800/4120, Training Loss Error:  1.587, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  62/100, Batch: 1900/4120, Training Loss Error:  1.547, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  62/100, Batch: 2000/4120, Training Loss Error:  1.559, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.784, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  62/100, Batch: 2100/4120, Training Loss Error:  1.557, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  62/100, Batch: 2200/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  62/100, Batch: 2300/4120, Training Loss Error:  1.631, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  62/100, Batch: 2400/4120, Training Loss Error:  1.612, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  62/100, Batch: 2500/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  62/100, Batch: 2600/4120, Training Loss Error:  1.642, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  62/100, Batch: 2700/4120, Training Loss Error:  1.653, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  62/100, Batch: 2800/4120, Training Loss Error:  1.641, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  62/100, Batch: 2900/4120, Training Loss Error:  1.630, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  62/100, Batch: 3100/4120, Training Loss Error:  1.691, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  62/100, Batch: 3200/4120, Training Loss Error:  1.583, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  62/100, Batch: 3300/4120, Training Loss Error:  1.607, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  62/100, Batch: 3400/4120, Training Loss Error:  1.638, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  62/100, Batch: 3500/4120, Training Loss Error:  1.606, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  62/100, Batch: 3600/4120, Training Loss Error:  1.600, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  62/100, Batch: 3700/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  62/100, Batch: 3800/4120, Training Loss Error:  1.587, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  62/100, Batch: 3900/4120, Training Loss Error:  1.641, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  62/100, Batch: 4000/4120, Training Loss Error:  1.622, Training Time on 100 Batches: 37 seconds\n",
            "Epoch:  62/100, Batch: 4100/4120, Training Loss Error:  1.629, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.822, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  63/100, Batch:    0/4120, Training Loss Error:  0.314, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  63/100, Batch:  100/4120, Training Loss Error:  1.571, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  63/100, Batch:  200/4120, Training Loss Error:  1.559, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  63/100, Batch:  300/4120, Training Loss Error:  1.540, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  63/100, Batch:  400/4120, Training Loss Error:  1.496, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  63/100, Batch:  500/4120, Training Loss Error:  1.539, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  63/100, Batch:  600/4120, Training Loss Error:  1.519, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  63/100, Batch:  700/4120, Training Loss Error:  1.517, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  63/100, Batch:  800/4120, Training Loss Error:  1.545, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  63/100, Batch:  900/4120, Training Loss Error:  1.543, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  63/100, Batch: 1000/4120, Training Loss Error:  1.558, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  63/100, Batch: 1100/4120, Training Loss Error:  1.560, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  63/100, Batch: 1200/4120, Training Loss Error:  1.547, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  63/100, Batch: 1300/4120, Training Loss Error:  1.559, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  63/100, Batch: 1400/4120, Training Loss Error:  1.528, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  63/100, Batch: 1500/4120, Training Loss Error:  1.552, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  63/100, Batch: 1600/4120, Training Loss Error:  1.543, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  63/100, Batch: 1700/4120, Training Loss Error:  1.571, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  63/100, Batch: 1800/4120, Training Loss Error:  1.576, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  63/100, Batch: 1900/4120, Training Loss Error:  1.537, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  63/100, Batch: 2000/4120, Training Loss Error:  1.547, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.792, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  63/100, Batch: 2100/4120, Training Loss Error:  1.543, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  63/100, Batch: 2200/4120, Training Loss Error:  1.654, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  63/100, Batch: 2300/4120, Training Loss Error:  1.616, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  63/100, Batch: 2400/4120, Training Loss Error:  1.599, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  63/100, Batch: 2500/4120, Training Loss Error:  1.626, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  63/100, Batch: 2600/4120, Training Loss Error:  1.627, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  63/100, Batch: 2700/4120, Training Loss Error:  1.642, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  63/100, Batch: 2800/4120, Training Loss Error:  1.633, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  63/100, Batch: 2900/4120, Training Loss Error:  1.618, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  63/100, Batch: 3000/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  63/100, Batch: 3100/4120, Training Loss Error:  1.680, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  63/100, Batch: 3200/4120, Training Loss Error:  1.570, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  63/100, Batch: 3300/4120, Training Loss Error:  1.593, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  63/100, Batch: 3400/4120, Training Loss Error:  1.625, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  63/100, Batch: 3500/4120, Training Loss Error:  1.592, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  63/100, Batch: 3600/4120, Training Loss Error:  1.588, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  63/100, Batch: 3700/4120, Training Loss Error:  1.625, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  63/100, Batch: 3800/4120, Training Loss Error:  1.577, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  63/100, Batch: 3900/4120, Training Loss Error:  1.627, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  63/100, Batch: 4000/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  63/100, Batch: 4100/4120, Training Loss Error:  1.617, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.833, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  64/100, Batch:    0/4120, Training Loss Error:  0.311, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  64/100, Batch:  100/4120, Training Loss Error:  1.557, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  64/100, Batch:  200/4120, Training Loss Error:  1.547, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  64/100, Batch:  300/4120, Training Loss Error:  1.523, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  64/100, Batch:  400/4120, Training Loss Error:  1.484, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  64/100, Batch:  500/4120, Training Loss Error:  1.528, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  64/100, Batch:  600/4120, Training Loss Error:  1.507, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  64/100, Batch:  700/4120, Training Loss Error:  1.510, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  64/100, Batch:  800/4120, Training Loss Error:  1.532, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  64/100, Batch:  900/4120, Training Loss Error:  1.532, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  64/100, Batch: 1000/4120, Training Loss Error:  1.542, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  64/100, Batch: 1100/4120, Training Loss Error:  1.552, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  64/100, Batch: 1200/4120, Training Loss Error:  1.534, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  64/100, Batch: 1300/4120, Training Loss Error:  1.549, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  64/100, Batch: 1400/4120, Training Loss Error:  1.515, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  64/100, Batch: 1500/4120, Training Loss Error:  1.540, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  64/100, Batch: 1600/4120, Training Loss Error:  1.533, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  64/100, Batch: 1700/4120, Training Loss Error:  1.551, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  64/100, Batch: 1800/4120, Training Loss Error:  1.561, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  64/100, Batch: 1900/4120, Training Loss Error:  1.527, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  64/100, Batch: 2000/4120, Training Loss Error:  1.537, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.800, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  64/100, Batch: 2100/4120, Training Loss Error:  1.534, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  64/100, Batch: 2200/4120, Training Loss Error:  1.645, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  64/100, Batch: 2300/4120, Training Loss Error:  1.600, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  64/100, Batch: 2400/4120, Training Loss Error:  1.591, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  64/100, Batch: 2500/4120, Training Loss Error:  1.614, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  64/100, Batch: 2600/4120, Training Loss Error:  1.619, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  64/100, Batch: 2700/4120, Training Loss Error:  1.632, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  64/100, Batch: 2800/4120, Training Loss Error:  1.621, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  64/100, Batch: 2900/4120, Training Loss Error:  1.606, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  64/100, Batch: 3000/4120, Training Loss Error:  1.598, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  64/100, Batch: 3100/4120, Training Loss Error:  1.665, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  64/100, Batch: 3200/4120, Training Loss Error:  1.556, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  64/100, Batch: 3300/4120, Training Loss Error:  1.580, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  64/100, Batch: 3400/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  64/100, Batch: 3500/4120, Training Loss Error:  1.580, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  64/100, Batch: 3600/4120, Training Loss Error:  1.577, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  64/100, Batch: 3700/4120, Training Loss Error:  1.614, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  64/100, Batch: 3800/4120, Training Loss Error:  1.562, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  64/100, Batch: 3900/4120, Training Loss Error:  1.617, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  64/100, Batch: 4000/4120, Training Loss Error:  1.598, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  64/100, Batch: 4100/4120, Training Loss Error:  1.601, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.840, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  65/100, Batch:    0/4120, Training Loss Error:  0.310, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  65/100, Batch:  100/4120, Training Loss Error:  1.550, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  65/100, Batch:  200/4120, Training Loss Error:  1.566, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  65/100, Batch:  300/4120, Training Loss Error:  1.528, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  65/100, Batch:  400/4120, Training Loss Error:  1.481, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  65/100, Batch:  500/4120, Training Loss Error:  1.525, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  65/100, Batch:  600/4120, Training Loss Error:  1.498, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  65/100, Batch:  700/4120, Training Loss Error:  1.497, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  65/100, Batch:  800/4120, Training Loss Error:  1.520, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  65/100, Batch:  900/4120, Training Loss Error:  1.518, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  65/100, Batch: 1000/4120, Training Loss Error:  1.532, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  65/100, Batch: 1100/4120, Training Loss Error:  1.536, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  65/100, Batch: 1200/4120, Training Loss Error:  1.519, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  65/100, Batch: 1300/4120, Training Loss Error:  1.534, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  65/100, Batch: 1400/4120, Training Loss Error:  1.502, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  65/100, Batch: 1500/4120, Training Loss Error:  1.525, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  65/100, Batch: 1600/4120, Training Loss Error:  1.513, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  65/100, Batch: 1700/4120, Training Loss Error:  1.537, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  65/100, Batch: 1800/4120, Training Loss Error:  1.543, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  65/100, Batch: 1900/4120, Training Loss Error:  1.508, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  65/100, Batch: 2000/4120, Training Loss Error:  1.517, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.808, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  65/100, Batch: 2100/4120, Training Loss Error:  1.514, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  65/100, Batch: 2200/4120, Training Loss Error:  1.624, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  65/100, Batch: 2300/4120, Training Loss Error:  1.585, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  65/100, Batch: 2400/4120, Training Loss Error:  1.573, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  65/100, Batch: 2500/4120, Training Loss Error:  1.601, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  65/100, Batch: 2600/4120, Training Loss Error:  1.601, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  65/100, Batch: 2700/4120, Training Loss Error:  1.609, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  65/100, Batch: 2800/4120, Training Loss Error:  1.602, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  65/100, Batch: 2900/4120, Training Loss Error:  1.585, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  65/100, Batch: 3000/4120, Training Loss Error:  1.584, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  65/100, Batch: 3100/4120, Training Loss Error:  1.645, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  65/100, Batch: 3200/4120, Training Loss Error:  1.539, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  65/100, Batch: 3300/4120, Training Loss Error:  1.562, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  65/100, Batch: 3400/4120, Training Loss Error:  1.591, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  65/100, Batch: 3500/4120, Training Loss Error:  1.559, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  65/100, Batch: 3600/4120, Training Loss Error:  1.558, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  65/100, Batch: 3700/4120, Training Loss Error:  1.591, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  65/100, Batch: 3800/4120, Training Loss Error:  1.542, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  65/100, Batch: 3900/4120, Training Loss Error:  1.587, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  65/100, Batch: 4000/4120, Training Loss Error:  1.572, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  65/100, Batch: 4100/4120, Training Loss Error:  1.584, Training Time on 100 Batches: 40 seconds\n",
            "Validation Loss Error:  1.855, Batch Validation Time: 49 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  66/100, Batch:    0/4120, Training Loss Error:  0.305, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  66/100, Batch:  100/4120, Training Loss Error:  1.523, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  66/100, Batch:  200/4120, Training Loss Error:  1.725, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  66/100, Batch:  300/4120, Training Loss Error:  1.615, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  66/100, Batch:  400/4120, Training Loss Error:  1.563, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  66/100, Batch:  500/4120, Training Loss Error:  1.595, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  66/100, Batch:  600/4120, Training Loss Error:  1.562, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  66/100, Batch:  700/4120, Training Loss Error:  1.548, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  66/100, Batch:  800/4120, Training Loss Error:  1.616, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  66/100, Batch:  900/4120, Training Loss Error:  1.904, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  66/100, Batch: 1000/4120, Training Loss Error:  2.106, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  66/100, Batch: 1100/4120, Training Loss Error:  2.008, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  66/100, Batch: 1200/4120, Training Loss Error:  1.945, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  66/100, Batch: 1300/4120, Training Loss Error:  1.953, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  66/100, Batch: 1400/4120, Training Loss Error:  1.807, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  66/100, Batch: 1500/4120, Training Loss Error:  1.788, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  66/100, Batch: 1600/4120, Training Loss Error:  1.751, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  66/100, Batch: 1700/4120, Training Loss Error:  1.751, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  66/100, Batch: 1800/4120, Training Loss Error:  1.734, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  66/100, Batch: 1900/4120, Training Loss Error:  1.700, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  66/100, Batch: 2000/4120, Training Loss Error:  1.698, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.807, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  66/100, Batch: 2100/4120, Training Loss Error:  1.684, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  66/100, Batch: 2200/4120, Training Loss Error:  1.793, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  66/100, Batch: 2300/4120, Training Loss Error:  1.741, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  66/100, Batch: 2400/4120, Training Loss Error:  1.734, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  66/100, Batch: 2500/4120, Training Loss Error:  1.747, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  66/100, Batch: 2600/4120, Training Loss Error:  1.738, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  66/100, Batch: 2700/4120, Training Loss Error:  1.753, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  66/100, Batch: 2800/4120, Training Loss Error:  1.744, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  66/100, Batch: 2900/4120, Training Loss Error:  1.717, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  66/100, Batch: 3000/4120, Training Loss Error:  1.724, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  66/100, Batch: 3100/4120, Training Loss Error:  1.784, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  66/100, Batch: 3200/4120, Training Loss Error:  1.665, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  66/100, Batch: 3300/4120, Training Loss Error:  1.682, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  66/100, Batch: 3400/4120, Training Loss Error:  1.710, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  66/100, Batch: 3500/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  66/100, Batch: 3600/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  66/100, Batch: 3700/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  66/100, Batch: 3800/4120, Training Loss Error:  1.646, Training Time on 100 Batches: 39 seconds\n",
            "Epoch:  66/100, Batch: 3900/4120, Training Loss Error:  1.692, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  66/100, Batch: 4000/4120, Training Loss Error:  1.668, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  66/100, Batch: 4100/4120, Training Loss Error:  1.715, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.787, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  67/100, Batch:    0/4120, Training Loss Error:  0.329, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  67/100, Batch:  100/4120, Training Loss Error:  1.639, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  67/100, Batch:  200/4120, Training Loss Error:  1.671, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  67/100, Batch:  300/4120, Training Loss Error:  1.641, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  67/100, Batch:  400/4120, Training Loss Error:  1.599, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  67/100, Batch:  500/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  67/100, Batch:  600/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  67/100, Batch:  700/4120, Training Loss Error:  1.605, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  67/100, Batch:  800/4120, Training Loss Error:  1.632, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  67/100, Batch:  900/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  67/100, Batch: 1000/4120, Training Loss Error:  1.642, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  67/100, Batch: 1100/4120, Training Loss Error:  1.650, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  67/100, Batch: 1200/4120, Training Loss Error:  1.628, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  67/100, Batch: 1300/4120, Training Loss Error:  1.648, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  67/100, Batch: 1400/4120, Training Loss Error:  1.606, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  67/100, Batch: 1500/4120, Training Loss Error:  1.628, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  67/100, Batch: 1600/4120, Training Loss Error:  1.617, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  67/100, Batch: 1700/4120, Training Loss Error:  1.643, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  67/100, Batch: 1800/4120, Training Loss Error:  1.644, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  67/100, Batch: 1900/4120, Training Loss Error:  1.608, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  67/100, Batch: 2000/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.824, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  67/100, Batch: 2100/4120, Training Loss Error:  1.602, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  67/100, Batch: 2200/4120, Training Loss Error:  1.703, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  67/100, Batch: 2300/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  67/100, Batch: 2400/4120, Training Loss Error:  1.638, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  67/100, Batch: 2500/4120, Training Loss Error:  2.035, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  67/100, Batch: 2600/4120, Training Loss Error:  2.467, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  67/100, Batch: 2700/4120, Training Loss Error:  2.086, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  67/100, Batch: 2800/4120, Training Loss Error:  1.916, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  67/100, Batch: 2900/4120, Training Loss Error:  1.820, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  67/100, Batch: 3000/4120, Training Loss Error:  1.795, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  67/100, Batch: 3100/4120, Training Loss Error:  1.865, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  67/100, Batch: 3200/4120, Training Loss Error:  1.755, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  67/100, Batch: 3300/4120, Training Loss Error:  1.771, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  67/100, Batch: 3400/4120, Training Loss Error:  1.796, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  67/100, Batch: 3500/4120, Training Loss Error:  1.766, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  67/100, Batch: 3600/4120, Training Loss Error:  1.753, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  67/100, Batch: 3700/4120, Training Loss Error:  1.786, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  67/100, Batch: 3800/4120, Training Loss Error:  1.729, Training Time on 100 Batches: 39 seconds\n",
            "Epoch:  67/100, Batch: 3900/4120, Training Loss Error:  1.777, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  67/100, Batch: 4000/4120, Training Loss Error:  1.758, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  67/100, Batch: 4100/4120, Training Loss Error:  1.764, Training Time on 100 Batches: 38 seconds\n",
            "Validation Loss Error:  1.825, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  68/100, Batch:    0/4120, Training Loss Error:  0.339, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  68/100, Batch:  100/4120, Training Loss Error:  1.697, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  68/100, Batch:  200/4120, Training Loss Error:  1.708, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  68/100, Batch:  300/4120, Training Loss Error:  1.696, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  68/100, Batch:  400/4120, Training Loss Error:  1.652, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  68/100, Batch:  500/4120, Training Loss Error:  1.694, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  68/100, Batch:  600/4120, Training Loss Error:  1.675, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  68/100, Batch:  700/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  68/100, Batch:  800/4120, Training Loss Error:  1.687, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  68/100, Batch:  900/4120, Training Loss Error:  1.696, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  68/100, Batch: 1000/4120, Training Loss Error:  1.691, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  68/100, Batch: 1100/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  68/100, Batch: 1200/4120, Training Loss Error:  1.675, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  68/100, Batch: 1300/4120, Training Loss Error:  1.695, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  68/100, Batch: 1400/4120, Training Loss Error:  1.653, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  68/100, Batch: 1500/4120, Training Loss Error:  1.674, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  68/100, Batch: 1600/4120, Training Loss Error:  1.660, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  68/100, Batch: 1700/4120, Training Loss Error:  1.687, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  68/100, Batch: 1800/4120, Training Loss Error:  1.676, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  68/100, Batch: 1900/4120, Training Loss Error:  1.644, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  68/100, Batch: 2000/4120, Training Loss Error:  1.656, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.870, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  68/100, Batch: 2100/4120, Training Loss Error:  1.638, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  68/100, Batch: 2200/4120, Training Loss Error:  1.737, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  68/100, Batch: 2300/4120, Training Loss Error:  1.692, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  68/100, Batch: 2400/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  68/100, Batch: 2500/4120, Training Loss Error:  1.696, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  68/100, Batch: 2600/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  68/100, Batch: 2700/4120, Training Loss Error:  1.713, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  68/100, Batch: 2800/4120, Training Loss Error:  1.698, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  68/100, Batch: 2900/4120, Training Loss Error:  1.683, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  68/100, Batch: 3000/4120, Training Loss Error:  1.671, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  68/100, Batch: 3100/4120, Training Loss Error:  1.736, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  68/100, Batch: 3200/4120, Training Loss Error:  1.624, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  68/100, Batch: 3300/4120, Training Loss Error:  1.648, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  68/100, Batch: 3400/4120, Training Loss Error:  1.676, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  68/100, Batch: 3500/4120, Training Loss Error:  1.643, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  68/100, Batch: 3600/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  68/100, Batch: 3700/4120, Training Loss Error:  1.667, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  68/100, Batch: 3800/4120, Training Loss Error:  1.610, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  68/100, Batch: 3900/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  68/100, Batch: 4000/4120, Training Loss Error:  1.638, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  68/100, Batch: 4100/4120, Training Loss Error:  1.647, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.865, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  69/100, Batch:    0/4120, Training Loss Error:  0.317, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  69/100, Batch:  100/4120, Training Loss Error:  1.593, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  69/100, Batch:  200/4120, Training Loss Error:  2.039, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  69/100, Batch:  300/4120, Training Loss Error:  1.864, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  69/100, Batch:  400/4120, Training Loss Error:  1.739, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  69/100, Batch:  500/4120, Training Loss Error:  1.760, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  69/100, Batch:  600/4120, Training Loss Error:  1.724, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  69/100, Batch:  700/4120, Training Loss Error:  1.718, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  69/100, Batch:  800/4120, Training Loss Error:  1.728, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  69/100, Batch:  900/4120, Training Loss Error:  1.735, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  69/100, Batch: 1000/4120, Training Loss Error:  1.715, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  69/100, Batch: 1100/4120, Training Loss Error:  1.733, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  69/100, Batch: 1200/4120, Training Loss Error:  1.702, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  69/100, Batch: 1300/4120, Training Loss Error:  1.717, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  69/100, Batch: 1400/4120, Training Loss Error:  1.670, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  69/100, Batch: 1500/4120, Training Loss Error:  1.691, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  69/100, Batch: 1600/4120, Training Loss Error:  1.684, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  69/100, Batch: 1700/4120, Training Loss Error:  1.703, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  69/100, Batch: 1800/4120, Training Loss Error:  1.706, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  69/100, Batch: 1900/4120, Training Loss Error:  1.662, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  69/100, Batch: 2000/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.793, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  69/100, Batch: 2100/4120, Training Loss Error:  1.670, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  69/100, Batch: 2200/4120, Training Loss Error:  1.776, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  69/100, Batch: 2300/4120, Training Loss Error:  1.722, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  69/100, Batch: 2400/4120, Training Loss Error:  1.707, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  69/100, Batch: 2500/4120, Training Loss Error:  1.792, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  69/100, Batch: 2600/4120, Training Loss Error:  1.806, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  69/100, Batch: 2700/4120, Training Loss Error:  1.844, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  69/100, Batch: 2800/4120, Training Loss Error:  1.789, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  69/100, Batch: 2900/4120, Training Loss Error:  1.787, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  69/100, Batch: 3000/4120, Training Loss Error:  1.839, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  69/100, Batch: 3100/4120, Training Loss Error:  1.887, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  69/100, Batch: 3200/4120, Training Loss Error:  1.743, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  69/100, Batch: 3300/4120, Training Loss Error:  1.762, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  69/100, Batch: 3400/4120, Training Loss Error:  1.804, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  69/100, Batch: 3500/4120, Training Loss Error:  1.777, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  69/100, Batch: 3600/4120, Training Loss Error:  1.730, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  69/100, Batch: 3700/4120, Training Loss Error:  1.762, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  69/100, Batch: 3800/4120, Training Loss Error:  1.710, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  69/100, Batch: 3900/4120, Training Loss Error:  1.754, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  69/100, Batch: 4000/4120, Training Loss Error:  1.732, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  69/100, Batch: 4100/4120, Training Loss Error:  1.734, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.865, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  70/100, Batch:    0/4120, Training Loss Error:  0.339, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  70/100, Batch:  100/4120, Training Loss Error:  1.678, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  70/100, Batch:  200/4120, Training Loss Error:  1.731, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  70/100, Batch:  300/4120, Training Loss Error:  1.685, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  70/100, Batch:  400/4120, Training Loss Error:  1.638, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  70/100, Batch:  500/4120, Training Loss Error:  1.686, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  70/100, Batch:  600/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  70/100, Batch:  700/4120, Training Loss Error:  1.644, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  70/100, Batch:  800/4120, Training Loss Error:  1.665, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  70/100, Batch:  900/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  70/100, Batch: 1000/4120, Training Loss Error:  1.676, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  70/100, Batch: 1100/4120, Training Loss Error:  1.690, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  70/100, Batch: 1200/4120, Training Loss Error:  1.664, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  70/100, Batch: 1300/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  70/100, Batch: 1400/4120, Training Loss Error:  1.631, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  70/100, Batch: 1500/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  70/100, Batch: 1600/4120, Training Loss Error:  1.649, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  70/100, Batch: 1700/4120, Training Loss Error:  1.669, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  70/100, Batch: 1800/4120, Training Loss Error:  1.666, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  70/100, Batch: 1900/4120, Training Loss Error:  1.639, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  70/100, Batch: 2000/4120, Training Loss Error:  1.640, Training Time on 100 Batches: 30 seconds\n",
            "Validation Loss Error:  1.889, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  70/100, Batch: 2100/4120, Training Loss Error:  1.629, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  70/100, Batch: 2200/4120, Training Loss Error:  1.726, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  70/100, Batch: 2300/4120, Training Loss Error:  1.680, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  70/100, Batch: 2400/4120, Training Loss Error:  1.658, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  70/100, Batch: 2500/4120, Training Loss Error:  1.691, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  70/100, Batch: 2600/4120, Training Loss Error:  1.688, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  70/100, Batch: 2700/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  70/100, Batch: 2800/4120, Training Loss Error:  1.690, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  70/100, Batch: 2900/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  70/100, Batch: 3000/4120, Training Loss Error:  1.663, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  70/100, Batch: 3100/4120, Training Loss Error:  1.729, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  70/100, Batch: 3200/4120, Training Loss Error:  1.620, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  70/100, Batch: 3300/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  70/100, Batch: 3400/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  70/100, Batch: 3500/4120, Training Loss Error:  1.642, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  70/100, Batch: 3600/4120, Training Loss Error:  1.632, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:  70/100, Batch: 3700/4120, Training Loss Error:  1.664, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  70/100, Batch: 3800/4120, Training Loss Error:  1.606, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  70/100, Batch: 3900/4120, Training Loss Error:  1.655, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  70/100, Batch: 4000/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  70/100, Batch: 4100/4120, Training Loss Error:  1.646, Training Time on 100 Batches: 40 seconds\n",
            "Validation Loss Error:  1.947, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  71/100, Batch:    0/4120, Training Loss Error:  0.317, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  71/100, Batch:  100/4120, Training Loss Error:  1.597, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  71/100, Batch:  200/4120, Training Loss Error:  1.688, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  71/100, Batch:  300/4120, Training Loss Error:  1.615, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  71/100, Batch:  400/4120, Training Loss Error:  1.572, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  71/100, Batch:  500/4120, Training Loss Error:  1.614, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  71/100, Batch:  600/4120, Training Loss Error:  1.588, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  71/100, Batch:  700/4120, Training Loss Error:  1.575, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  71/100, Batch:  800/4120, Training Loss Error:  1.599, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  71/100, Batch:  900/4120, Training Loss Error:  1.607, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  71/100, Batch: 1000/4120, Training Loss Error:  1.603, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  71/100, Batch: 1100/4120, Training Loss Error:  1.613, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  71/100, Batch: 1200/4120, Training Loss Error:  1.593, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  71/100, Batch: 1300/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  71/100, Batch: 1400/4120, Training Loss Error:  1.602, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  71/100, Batch: 1500/4120, Training Loss Error:  1.620, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  71/100, Batch: 1600/4120, Training Loss Error:  1.606, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  71/100, Batch: 1700/4120, Training Loss Error:  1.626, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  71/100, Batch: 1800/4120, Training Loss Error:  1.626, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  71/100, Batch: 1900/4120, Training Loss Error:  1.589, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  71/100, Batch: 2000/4120, Training Loss Error:  1.595, Training Time on 100 Batches: 30 seconds\n",
            "Validation Loss Error:  1.809, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  71/100, Batch: 2100/4120, Training Loss Error:  1.585, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  71/100, Batch: 2200/4120, Training Loss Error:  1.686, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  71/100, Batch: 2300/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  71/100, Batch: 2400/4120, Training Loss Error:  1.635, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  71/100, Batch: 2500/4120, Training Loss Error:  1.666, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  71/100, Batch: 2600/4120, Training Loss Error:  1.661, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  71/100, Batch: 2700/4120, Training Loss Error:  1.670, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  71/100, Batch: 2800/4120, Training Loss Error:  1.659, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  71/100, Batch: 2900/4120, Training Loss Error:  1.646, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  71/100, Batch: 3000/4120, Training Loss Error:  1.634, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  71/100, Batch: 3100/4120, Training Loss Error:  1.698, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  71/100, Batch: 3200/4120, Training Loss Error:  1.594, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  71/100, Batch: 3300/4120, Training Loss Error:  1.609, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  71/100, Batch: 3400/4120, Training Loss Error:  1.641, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  71/100, Batch: 3500/4120, Training Loss Error:  1.604, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  71/100, Batch: 3600/4120, Training Loss Error:  1.600, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  71/100, Batch: 3700/4120, Training Loss Error:  1.634, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  71/100, Batch: 3800/4120, Training Loss Error:  1.583, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:  71/100, Batch: 3900/4120, Training Loss Error:  1.626, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:  71/100, Batch: 4000/4120, Training Loss Error:  1.609, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:  71/100, Batch: 4100/4120, Training Loss Error:  1.618, Training Time on 100 Batches: 39 seconds\n",
            "Validation Loss Error:  1.830, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  72/100, Batch:    0/4120, Training Loss Error:  0.313, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  72/100, Batch:  100/4120, Training Loss Error:  1.565, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  72/100, Batch:  200/4120, Training Loss Error:  1.613, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  72/100, Batch:  300/4120, Training Loss Error:  1.576, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  72/100, Batch:  400/4120, Training Loss Error:  1.529, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  72/100, Batch:  500/4120, Training Loss Error:  1.571, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  72/100, Batch:  600/4120, Training Loss Error:  1.548, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  72/100, Batch:  700/4120, Training Loss Error:  1.539, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  72/100, Batch:  800/4120, Training Loss Error:  1.564, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  72/100, Batch:  900/4120, Training Loss Error:  1.569, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  72/100, Batch: 1000/4120, Training Loss Error:  1.573, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  72/100, Batch: 1100/4120, Training Loss Error:  1.582, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  72/100, Batch: 1200/4120, Training Loss Error:  1.560, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  72/100, Batch: 1300/4120, Training Loss Error:  1.578, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  72/100, Batch: 1400/4120, Training Loss Error:  1.544, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  72/100, Batch: 1500/4120, Training Loss Error:  1.574, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  72/100, Batch: 1600/4120, Training Loss Error:  1.575, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  72/100, Batch: 1700/4120, Training Loss Error:  1.594, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  72/100, Batch: 1800/4120, Training Loss Error:  1.595, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  72/100, Batch: 1900/4120, Training Loss Error:  1.560, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  72/100, Batch: 2000/4120, Training Loss Error:  1.566, Training Time on 100 Batches: 30 seconds\n",
            "Validation Loss Error:  1.790, Batch Validation Time: 50 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  72/100, Batch: 2100/4120, Training Loss Error:  1.559, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  72/100, Batch: 2200/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  72/100, Batch: 2300/4120, Training Loss Error:  1.612, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  72/100, Batch: 2400/4120, Training Loss Error:  1.597, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  72/100, Batch: 2500/4120, Training Loss Error:  1.624, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  72/100, Batch: 2600/4120, Training Loss Error:  1.627, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  72/100, Batch: 2700/4120, Training Loss Error:  1.638, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  72/100, Batch: 2800/4120, Training Loss Error:  1.623, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  72/100, Batch: 2900/4120, Training Loss Error:  1.615, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  72/100, Batch: 3000/4120, Training Loss Error:  1.604, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  72/100, Batch: 3100/4120, Training Loss Error:  1.663, Training Time on 100 Batches: 29 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s5ArFS775An",
        "colab_type": "code",
        "outputId": "4b0b5846-dd7b-46bd-f980-763cf16046a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        "checkpoint = \"./chatbot_weights.ckpt\"\n",
        "session = tf.InteractiveSession()\n",
        "session.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(session, checkpoint)\n",
        " \n",
        "# Converting the questions from strings to lists of encoding integers\n",
        "def convert_string2int(question, word2int):\n",
        "    question = clean_text(question)\n",
        "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]\n",
        " \n",
        "# Setting up the chat\n",
        "while(True):\n",
        "    question = input(\"You: \")\n",
        "    if question == 'Goodbye':\n",
        "        break\n",
        "    question = convert_string2int(question, questionswords2int)\n",
        "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
        "    fake_batch = np.zeros((batch_size, 25))\n",
        "    fake_batch[0] = question\n",
        "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
        "    answer = ''\n",
        "    for i in np.argmax(predicted_answer, 1):\n",
        "        if answersints2word[i] == 'i':\n",
        "            token = ' I'\n",
        "        elif answersints2word[i] == '<EOS>':\n",
        "            token = '.'\n",
        "        elif answersints2word[i] == '<OUT>':\n",
        "            token = 'out'\n",
        "        else:\n",
        "            token = ' ' + answersints2word[i]\n",
        "        answer += token\n",
        "        if token == '.':\n",
        "            break\n",
        "    print('ChatBot: ' + answer)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a14205415657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Setting up the chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Goodbye'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}